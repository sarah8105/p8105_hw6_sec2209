---
title: "P8105 Homework 6"
author: "sarah_8105"
output: github_document
---


This is my sixth homework assignment for P8105.

```{r libraries}
library(tidyverse)
library(rvest)
library(readr)
library(httr)
library(modelr)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


## Problem 1

In this first code chunk, I pull the Washington Post data on homicides in 50 large US cities. I create a `city_state` variable that concatenates the city and state that the homicide occurred in and remove an observation that was erroneously attributed to Tulsa, AL. I also create a resolution variable to classify homicides that were closed by arrest versus those that had no arrest and filter to homicides with white or black victims due to low numbers among other race/ethnicities.

```{r p1_import, cache = TRUE}
homicide_df = read_csv(file = "https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv") %>%
  mutate(
    city_state = str_c(city, state, sep = ", "),
    victim_age = as.numeric(victim_age),
    resolution = case_when(
      disposition == "Closed without arrest" ~ 0,
      disposition == "Open/No arrest" ~ 0,
      disposition == "Closed by arrest" ~ 1
    )
  ) %>%
  select(city_state, resolution, victim_age, victim_race, victim_sex) %>%
  filter(
    city_state != "Tulsa, AL",
    victim_race %in% c("White", "Black")
    )
```

In the following code chunk, I run a logistic regression model of the homicide resolution by victim age, race, and sex for the city of Baltimore, MD.
```{r p1_md}
baltimore_df =
  homicide_df %>%
  filter(city_state == "Baltimore, MD")

glm(resolution ~ victim_age + victim_race + victim_sex,
    data = baltimore_df,
    family = binomial()) %>%
  broom::tidy() %>%
  mutate(
    OR = exp(estimate),
    CI_lower = exp(estimate - 1.96 * std.error),
    CI_upper = exp(estimate + 1.96 * std.error)
  ) %>%
  select(term, OR, starts_with("CI")) %>%
  knitr::kable(digits = 3)
```
Based on the model results, the odds of a homicide being resolved in Baltimore are over two-times as high among white victims as compared to black victims after controlling for victim age and sex.

In the next code chunk, I iterate this logistic model over all cities included in the data.

```{r p1_models}
models_results_df = 
  homicide_df %>% 
  nest(data = -city_state) %>% 
  mutate(
    models = 
      map(.x = data, ~glm(resolution ~ victim_age + victim_race + victim_sex, data = .x, family = binomial())),
    results = map(models, broom::tidy)
  ) %>% 
  select(city_state, results) %>% 
  unnest(results) %>% 
  mutate(
    OR = exp(estimate),
    CI_lower = exp(estimate - 1.96 * std.error),
    CI_upper = exp(estimate + 1.96 * std.error)
  ) %>% 
  select(city_state, term, OR, starts_with("CI")) 
```

I then graph the odds ratios and 95% confidence intervals for the race coefficient by city.

```{r p1_plot}
models_results_df %>%
  filter(term == "victim_raceWhite") %>%
  mutate(city_state = fct_reorder(city_state, OR)) %>%
  ggplot(aes(x = city_state, y = OR)) +
  geom_point() +
  geom_errorbar(aes(ymin = CI_lower, ymax = CI_upper)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))  +
  labs(
    title = "Adjusted OR for Solving Homicides among White vs. Black Victims",
    x = "City",
    y = "Odds Ratio"
  ) +
  geom_hline(
    color = "blue",
    yintercept = 1
  ) 

```

There is wide variation in the estimates across cities. Most cities have an odds ratio above 1 (indicating higher odds of solving the homicide for white victims as compared to black victims), but a handful have an odds ratio below 1 (though the confidence interval for these cities cross above 1). Oakland, CA, Omaha, NE, and Boston, MA have the strongest association between victim race and solving the homicide after controlling for victim age and sex. 


## Problem 2

Problem 2 focuses on understanding the effects of different variables on child birth weight. The data set consists of ~4000 children and factors related to their birth and their parents. In this first code chunk, I import and explore the data to assess distributions and missingness in the data. I also modify the factor variables (`babysex`: baby sex, `frace`: father's race, `mrace`: mother's race, and `malform`: presence of malformations) to display the actual categorical options instead of the numeric codes.


```{r bwt_import}
bwt_df = 
  read_csv("Data/birthweight.csv") %>%
  janitor::clean_names() %>%
  mutate(
    babysex = as_factor(babysex),
    frace = as_factor(frace),
    malform = as_factor(malform),
    mrace = as_factor(mrace),
    frace = recode(frace, "1" = "White", "2" = "Black", 
           "3" = "Asian", "4" = "Puerto Rican", "8" = "Other", "9" = "Unknown"),
    mrace = recode(mrace, "1" = "White", "2" = "Black", 
           "3" = "Asian", "4" = "Puerto Rican", "8" = "Other"),
    babysex = recode(babysex, "1" = "Male", "2" = "Female"),
    malform = recode(malform, "1" = "Present", "0" = "Absent")
  )  

summary(bwt_df) 

bwt_df =
  bwt_df %>%
  select(-pnumlbw, -pnumsga, -parity, -smoken, -menarche, -fincome)

```

At first glance, there appears to be no missing data in this data set, but it's possible that some of the zeros represent missing data. For example, an age of menarchy of 0 is likely missing or erroneous. However, it's possible that some zeros are valid entries. For example, an average number of cigarettes smoked of 0 likely indicates that the mother was not a smoker and a parity of 0 could indicate that this is the first live birth for the mother. Since there is no additional information about how these variables were collected or recorded and I cannot distinguish true zeros from zeros that represent missing data, I dropped all variables that had at least one record with a zero.


In the next code chunk, I run and assess the fit of the first linear regression model on birth weight:  
 
  * *Model 1:* main effects of length at birth and gestational age only


```{r bwt_model1}
model1 = lm(bwt ~ blength + gaweeks, data = bwt_df)
broom::glance(model1)
broom::tidy(model1)

bwt_df %>%
  modelr::add_predictions(model1) %>%
  modelr::add_residuals(model1) %>%
  ggplot(aes(x = pred, y = resid)) +
  geom_point(alpha = .5) +
  labs(
    title = "Model 1: Main Effects",
    x = "Predicted Birth Weight",
    y = "Residual"
  )

```

Based on these results, birth length and gestational age are significant predictors of birth weight. However, there appears to be a skewed residual distribution, with a slight funnel shape as the predicted birth weight increases and poor fit for some outlying values.  

In the next code chunk, I run and assess the fit of the second linear regression model on birth weight:  
 
  * *Model 2:* including predictors of head circumference, length, sex, and all interactions between these variables

```{r bwt_model2}
model2 = lm(bwt ~ babysex*bhead*blength, data = bwt_df)
broom::glance(model2)
broom::tidy(model2)

bwt_df %>%
  modelr::add_predictions(model2) %>%
  modelr::add_residuals(model2) %>%
  ggplot(aes(x = pred, y = resid)) +
  geom_point(alpha = .5) +
  labs(
    title = "Model 2: Interaction Effects",
    x = "Predicted Birth Weight",
    y = "Residual"
  )
```

This model still shows an even more distinct funnel shape in the residuals than was seen in model 1.

In the next code chunk, I run and assess the fit of the third linear regression model on birth weight, which was informed by the previous two models and through a review of significant predictors of low birth weight in the literature:  
  
  * *Model 3:* predictors in model 1 & 2, in addition to mother's race, baby's sex, weight gain, and mother's pre-pregnancy BMI

```{r bwt_model3}
model3 = lm(bwt ~ blength + gaweeks + babysex*bhead*blength + mrace + babysex + ppbmi + wtgain, data = bwt_df)
broom::glance(model3)
broom::tidy(model3)

bwt_df %>%
  modelr::add_predictions(model3) %>%
  modelr::add_residuals(model3) %>%
  ggplot(aes(x = pred, y = resid)) +
  geom_point(alpha = .5) +
  labs(
    title = "Model 3: Log Transformation and Evidence-Based Predictors",
    x = "Predicted Birth Weight",
    y = "Residual"
  )
```

This model still has a slight funnel shape in the residuals but appears to be a better fit than the previous two models.

Finally, I compare the three models illustrated above with cross validation using the `crossv_mc` function in the `modelr` package and 100 re-samplings of the training and test data sets. 

```{r bwt_cross}
cv_df =
  crossv_mc(bwt_df, 100) 

cv_df =
  cv_df %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))

cv_df =
  cv_df %>%
  mutate(
    model1 = map(.x = train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    model2 = map(.x = train, ~lm(bwt ~ babysex*bhead*blength, data = .x)),
    model3 = map(.x = train, ~lm(bwt ~ blength + gaweeks + babysex*bhead*blength + mrace + babysex + ppbmi + wtgain, data = .x))
  ) %>%
  mutate(
    rmse_model1 = map2_dbl(.x = model1, .y = test, ~rmse(model = .x, data = .y)),
    rmse_model2 = map2_dbl(.x = model2, .y = test, ~rmse(model = .x, data = .y)),
    rmse_model3 = map2_dbl(.x = model3, .y = test, ~rmse(model = .x, data = .y))
  ) 

cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse, fill = model)) +
  geom_violin(alpha = .5) +
  labs(
    title = "Comparison of RMSE using Cross Validation",
    x = "Model",
    y = "RMSE"
  ) +
  theme(legend.position = "none")
```

As seen in this plot, model 3 has the lowest distribution in RMSE values and therefore has the best fit based on this criteria. Model 2 also performs relatively well as compared to model 1.

## Problem 3

In this problem, I use 2017 Central Park weather data. First, I import the data using the provided P8105 code.

```{r weather_import, cache = TRUE}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

Next, I run a simple linear regression of maximum temperature (`tmax`) on minimum temperature (`tmin`) using 5000 bootstrap samples. I also calculate log(β^0 ∗ β^1) for each regression output, plot the distribution of this estimate for all 5000 samples, and calculate the mean and 95% confidence interval. 

```{r weather_boot}
boot_strap_results = 
  weather_df %>%
  modelr::bootstrap(n = 5000) %>%
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::tidy),
    r2 = map(models, broom::glance)) %>%
  select(-strap, -models) 

terms = 
  boot_strap_results %>%
  unnest(results) %>%
  select(.id, term, estimate) %>%
  pivot_wider(
    names_from = "term",
    values_from = "estimate"
  ) %>%
  janitor::clean_names() %>%
  mutate(
    estimate = log(intercept * tmin)
  ) 


terms %>%
  ggplot(aes(x = estimate)) + 
  geom_density() +
  labs(
    title = "Distribution in log(β^0 ∗ β^1) Using 5000 Bootstrap Samples",
    x = "log(β^0 ∗ β^1)",
    y = "Density"
  )

terms %>%
  summarize(
    Mean = mean(estimate),
    LCL = quantile(estimate, 0.025),
    UCL = quantile(estimate, 0.975)
    ) %>% 
  knitr::kable(digits = 2)
```

The distribution of log(β^0 ∗ β^1) follows the typical bell-shape. The mean of log(β^0 ∗ β^1) is 2.01 and the 95% confidence interval is 1.97-2.06. 

In this last code chunk, I plot the distribution of the R^2 for all 5000 samples and calculate the mean and 95% confidence interval.  

```{r p3_plots}
r2 = 
  boot_strap_results %>%
  unnest(r2) 


r2 %>%
  ggplot(aes(x = r.squared)) +
  geom_density() +
  labs(
    title = "Distribution in R^2 Using 5000 Bootstrap Samples",
    x = "R^2",
    y = "Density"
  )

r2 %>%
  summarize(
    Mean = mean(r.squared),
    LCL = quantile(r.squared, 0.025),
    UCL = quantile(r.squared, 0.975)
    ) %>% 
  knitr::kable(digits = 3)
```

The distribution of R^2 also follows the typical bell-shape. The mean of R^2 is 0.911 and the 95% confidence interval is 0.894-0.927. 